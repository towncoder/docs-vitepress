## 从订单系统入手进行MQ的改造

### 前情提要

![image-20220420115749447](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420115749447.png)

涉及技术点：MQ、Elasticsearch、分库分表、sql优化、rpc调用、服务限流、分布式锁、分布式事务

生产环境负载：

- 累积注册几千万
- 活跃的用户数量是一两百万的
- 每天新增的订单数量，目前大概是几十万
- 大促活动的时可以达到单日百万订单的量级
- QPS：每秒2000左右的访问量；活动时，会达到每秒1万以上。

整体压力：

一方面是订单系统日益增长的数据量 —— 分库分表

一方面是在大促活动时每秒上万的访问压力 —— 削峰、限流

### 核心问题

#### 问题一：下单后的修改订单状态，发送通知，发放优惠券过程太复杂。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420144145580.png)

#### 问题二：退款都做了那些操作？

- 重新给商品增加库存

- 更新订单状态为“已完成”

- 减少你的积分收回你的优惠券和红包

- 发送Push告诉你退款完成了

- 通知仓储系统取消发货
- 通过第三方支付系统把钱重新退还给你

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420144831524.png)

最大问题：第三方支付系统如果退款失败怎么办？

#### 问题三：如果用户下单后一直不付款怎么办？

启动一个后台线程，这个后台线程就是专门扫描数据库里那些待付款的订单。如果发现超过24小时还没付款，就直接把订单状态改成“已关闭”了，释放掉锁定的那些商品库存。

如果有几十万订单没付款，难道要一直傻傻的扫描？？？—— 延时队列

#### 问题四：老司机设计系统的必备经验：跟第三方系统打交道

比如：商品的出库发货，你找谁？

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420152436281.png)

跟第三方系统耦合的痛苦：性能差，不稳定。带来很多隐患问题 —— MQ解耦

#### 问题五：大数据团队在做什么？

用户积累下来的一些浏览行为、访问行为、交易行为都是各种数据，这个数据量很大，所以你可以称之为“大数据”。

大数据团队每天要负责的事情，说白了就是去**尽可能的搜集每天100万用户在你的APP上的各种行为数据。**

大数据团队搜集过来大量的数据之后，就形成了所谓的“大数据”。接着他用这些大数据可以计算出很多东西。最常见的就是数据报表，比如说用户行为报表，订单分析报表，等等。这些数据报表都是提供给老板来看的。

大数据团队从我们这里提取数据，已经**严重影响到我们订单系统的运行**了。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420160330083.png)

#### 问题六：双11对一个订单系统到底有多大压力？

做一个设想，如果有200万用户参与双11活动，在双11购物最高峰的时候，肯定会比往年的高峰QPS高好几倍，预计有可能今年双11最高峰的时候，会达到每秒至少1万的QPS。

也就是说，光是系统被请求的QPS就会达到1万以上，那么系统请求数据库的QPS就会达到2万以上。仅仅凭借我们目前的数据库性能，是无论如何扛不住每秒2万请求的。

QPS统计框架或监控系统观察自己系统的QPS。

>上面其实列出了MQ的三大应用场景，也是主要作用：异步、解耦、削峰。
>
>总结如下
>
>![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420161829904.png)

### 消息中间件引入

#### MQ是什么？

1.异步调用

消息中间件，其实就是一种系统，他自己也是独立部署的，然后让我们的两个系统之间通过发消息和收消息，来进行异步的调用，而不是仅仅局限于同步调用。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420162850928.png)

2.解耦

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420163613858.png)

3.流量削峰

系统A发送过来的每秒1万请求是一个流量洪峰，然后MQ直接给扛下来了，都存储自己本地磁盘，这个过程就是流量削峰的过程，瞬间把一个洪峰给削下来了，让系统B后续慢慢获取消息来处理。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420163814762.png)

#### MQ技术选型

**选型标准**

业内常用的MQ有哪些？每一种MQ各自的表现如何？这些MQ在同等机器条件下，能抗多少QPS（每秒抗几千QPS还是几万QPS）？性能有多高（发送一条消息给他要2ms还是20ms）？可用性能不能得到保证（要是MQ部署的机器挂了怎么办）？然后你还得考虑：他们会不会丢失数据？如果需要的话能否让他们进行线性的集群扩容（就是多加几台机器）？消息中间件经常需要使用的一些功能他们都有吗（比如说延迟消息、事务消息、消息堆积、消息回溯、死信队列，等等）？另外还得考虑这些MQ在文档是否齐全？社区是否活跃？在行业内是否广泛运用？是用什么语言编写的？

**Kafka的优势和劣势**

- 首先Kafka的吞吐量几乎是行业里最优秀的，在常规的机器配置下，一台机器可以达到每秒十几万的QPS，相当的强悍。
- Kafka比较为人诟病的一点，似乎是丢数据方面的问题。
- Kafka另外一个比较大的缺点，就是功能非常的单一。
- 行业里的一个标准，是把Kafka用在用户行为日志的采集和传输上。

**RabbitMQ的优势和历史**

- RabbitMQ的优势在于可以保证数据不丢失，也能保证高可用性。
- 支持部分高级功能，比如说死信队列，消息重试之类的，这些是他的优点。
- 最为人诟病的，就是RabbitMQ的吞吐量是比较低的，一般就是每秒几万的级别。
- 进行集群扩展的时候（也就是加机器部署），还比较麻烦。
- 还有一个较为致命的缺陷，就是他的开发语言是erlang。

**RocketMQ的优势和劣势**

- 阿里开源的消息中间件，久经沙场，非常的靠谱。他几乎同时解决了Kafka和RabbitMQ的缺陷。
- RocketMQ的吞吐量也同样很高，单机可以达到10万QPS以上，而且可以保证高可用性，性能很高，
- 而且支持通过配置保证数据绝对不丢失，可以部署大规模的集群，
- 支持各种高级的功能，比如说延迟消息、事务消息、消息回溯、死信队列、消息积压，等等。
- RocketMQ的官方文档相对简单一些，但是Kafka和RabbitMQ的官方文档就非常的全面和详细，这可能是RocketMQ目前唯一的缺点。

> 从长远考虑，后面用RocketMQ重构订单系统。

### RocketMQ引入

**MQ如何集群化部署来支撑高并发访问？**

部署在多台机器上

todo

**MQ如果要存储海量消息应该怎么做？**

分布式的存储：把数据分散在多台机器上来存储，每台机器存储一部分消息，这样多台机器加起来就可以存储海量消息了！

**高可用保障：万一Broker宕机了怎么办？**

要是任何一台Broker突然宕机了怎么办？那不就会导致RocketMQ里一部分的消息就没了吗？这就会导致MQ的不可靠和不可用，这个问题怎么解决？

RocketMQ的解决思路是**Broker主从架构以及多副本策略。**

**数据路由：怎么知道访问哪个Broker？**

现在又有一个问题了，对于系统来说，要发送消息到MQ里去，还要从MQ里消费消息那么大家怎么知道有哪些Broker？怎么知道要连接到哪一台Broker上去发送和接收消息？

这是一个大问题！所以RocketMQ为了解决这个问题，有一个NameServer的概念，他也是独立部署在几台机器上的，然后所有的Broker都会把自己注册到NameServer上去，NameServer不就知道集群里有哪些Broker了？

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220420181915486.png)

#### 路由中心的架构原理

RocketMQ这个技术一共是包含了四个核心的部分：

- 第一块就是他的NameServer，这个东西很重要，他要负责去管理集群里所有Broker的信息，让使用MQ的系统可以通过他感知到集群里有哪些Broker。

- 第二块就是Broker集群本身了，必须得在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本存储和高可用。
- 第三块就是向MQ发送消息的那些系统了，这些系统一般称之为生产者，这里也有很多细节是值得深究的，因为这些生产者到底是如何从NameServer拉取路由信息的？如何选择Broker机器建立连接以及发送消息的？
- 第四块就是从MQ获取消息的那些系统，这些系统一般称之为消费者。

**NameServer到底可以部署几台机器？**

可以多台集群部署，每个Broker启动都得向所有的NameServer进行注册。

Broker 每隔一段时间向  NameServer 拉取最新的集群Broker信息。

类似一个微服务注册中心的架构。会有哪些问题呢？

#### 聊聊 Broker

**主从同步如何实现的？**

RocketMQ的Master-Slave模式采取的是Slave Broker不停的发送请求到Master Broker去拉取消息，采取的是Pull模式拉取消息。

**有读写分离吗？**

没有，有可能从Master Broker获取消息，也有可能从Slave Broker获取消息。

- 举个例子，要是这个时候Master Broker负载很重，回去 salve 查。
- 如果 salve 自己同步的很慢，去 Master 查。

**如果Slave Broke挂掉了有什么影响？**

影响不大，如果 master 挂掉可能会有影响。

有主从切换吗?

所以这种Master-Slave模式不是彻底的高可用模式，他没法实现自动把Slave切换为Master.。

在RocketMQ 4.5之后，这种情况得到了改变，因为RocketMQ支持了一种新的机制，叫做Dledger本身这个东西是基于Raft协议实现的一个机制，实现原理和算法思想是有点复杂的，我们在这里先不细说。

一旦Master Broker宕机了，就可以在多个副本，也就是多个Slave中，通过Dledger技术和Raft协议算法进行leader选举，直接将一个Slave Broker选举为新的Master Broker，然后这个新的Master Broker就可以对外提供服务了。

>  希望大家去研究一下Kafka和RabbitMQ的多副本和高可用机制，Kafka是如何在集群里维护多个副本的？出现故障的时候能否实现自动切换？RabbitMQ是如何在集群里维护多个数据副本的？出现故障的时候能否实现自动切换？  
>
> 既然有主从同步机制，那么有没有主从数据不一致的问题？Slave永远落后Master一些数据，这就是主从不一致。那么这种不一致有没有什么问题？有办法保证主从数据强制一致吗？这样做又会有什么缺点呢？                                   

#### 落地实现

1.NameServer集群化部署，保证高可用性。因为Broker要向每个NameServer都注册一遍，NameServer之间不需要通信，所以可以分别在不同机器上，以保证其高可用。

2.基于Dledger的Broker主从架构部署。 Dledger技术是要求至少得是一个Master带两个Slave，这样有三个Broke组成一个Group，也就是作为一个分组来运行。一旦Master宕机，他就可以从剩余的两个Slave中选举出来一个新的Master对外提供服务。

3.主从之间的数据同步问题。

Broker跟NameServer之间的通信：TCP长连接，Broker会跟每个NameServer都建立一个TCP长连接，然后定时通过TCP长连接发送心跳请求过去。

具体：Broker会每隔30秒发送心跳到所有的NameServer上去，然后每个NameServer都会每隔10s检查一次有没有哪个Broker超过120s没发送心跳的，如果有，就认为那个Broker已经宕机了，从路由信息里要摘除这个Broker。

4.MQ的核心数据模型：Topic到底是什么？其实他表达的意思就是一个数据集合的意思。

5.Topic数据在Broker集群是如何存储的？分布式存储：一个Topic存放在多个Broker里面，每个Broker在进行定时的心跳汇报给NameServer的时候，都会告诉NameServer自己当前的数据情况，比如有哪些Topic的哪些数据在自己这里，这些信息都是属于路由信息的一部分。

6.生产者系统是如何将消息发送给Broker的？

在发送消息之前，得先有一个Topic，找到topic之后，从NameServer获取到获取到Broker信息，再和Broker建立长链接发送消息。

消费者是如何从Broker上拉取消息的？消费者系统其实跟生产者系统原理是类似的，他们也会跟NameServer建立长连接，然后拉取路由信息，接着找到自己要获取消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220516144611724.png)

> 整体架构：
>
> - 多NameServer
>
> - 多主多从
>
> - 主从切换
>
> - 分布式存储

集群搭建练习：

已完成

#### 监控运维

>要看RocketMQ集群能承载的最高QPS。  —— RocketMQ DashBoard
>
>同时在承载这个QPS的同时，各个机器的CPU、IO、磁盘、网络、内存的负载情况，要看机器资源的使用率，还要看JVM的GC情况，等等。     —— 运维监控 top

如何监控运维一个RockeMQ集群？

642600657

1.OS内核参数调整

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220516145727414.png)

**vm.overcommit_memory**

这个参数有三个值可以选择，0、1、2。

如果值是0的话，在你的中间件系统申请内存的时候，os内核会检查可用内存是否足够，如果足够的话就分配内存给你，如果感觉剩余内存不是太够了，干脆就拒绝你的申请，导致你申请内存失败，进而导致中间件系统异常出错。

因此一般需要将这个参数的值调整为1，意思是把所有可用的物理内存都允许分配给你，只要有内存就给你来用，这样可以避免申请内存失败的问题。

比如我们曾经线上环境部署的Redis就因为这个参数是0，导致在save数据快照到磁盘文件的时候，需要申请大内存的时候被拒绝了，进而导致了异常报错。可以用如下命令修改：

```  shell
echo 'vm.overcommit_memory=1' >> /etc/sysctl.conf。
```

**vm.max_map_count**

这个参数的值会影响中间件系统可以开启的线程的数量，同样也是非常重要的

如果这个参数过小，有的时候可能会导致有些中间件无法开启足够的线程，进而导致报错，甚至中间件系统挂掉。

他的默认值是65536，但是这个值有时候是不够的，比如我们大数据团队的生产环境部署的Kafka集群曾经有一次就报出过这个异常，说无法开启足够多的线程，直接导致Kafka宕机了。

因此建议可以把这个参数调大10倍，比如655360这样的值，保证中间件可以开启足够多的线程。可以用如下命令修改：

```shell
echo 'vm.max_map_count=655360' >> /etc/sysctl.conf。
```

**vm.swappiness**





#### 压测

> 部分参数省略
>
> 网卡流量
>
> 使用如下命令可以查看服务器的网卡流量：
>
> sar -n DEV 1 2
>
> 通过这个命令就可以看到每秒钟网卡读写数据量了。当时我们的服务器使用的是千兆网卡，千兆网卡的理论上限是每秒传输128M数据，但是一般实际最大值是每秒传输100M数据。

到底应该如何压测：应该在TPS和机器的cpu负载、内存使用率、jvm gc频率、磁盘io负载、网络流量负载之间取得一个平衡，尽量让TPS尽可能的提高，同时让机器的各项资源负载不要太高。

实际压测过程：采用几台机器开启大量线程并发读写消息，然后观察TPS、cpu load（使用top命令）、内存使用率（使用free命令）、jvm gc频率（使用jstat命令）、磁盘io负载（使用top命令）、网卡流量负载（使用sar命令），不断增加机器和线程，让TPS不断提升上去，同时观察各项资源负载是否过高。

生产集群规划：根据公司的后台整体QPS来定，稍微多冗余部署一些机器即可，实际部署生产环境的集群时，使用高配置物理机，同时合理调整os内核参数、jvm参数、中间件核心参数，如此即可。

#### 生产集群总结复习

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220516175148047.png)

### 改造订单系统

>回顾订单系统的问题：
>
>- 下单核心流程环节太多，性能较差
>
>- 订单退款的流程可能面临退款失败的风险
>
>- 关闭过期订单的时候，存在扫描大量订单数据的问题
>
>- 跟第三方物流系统耦合在一起，性能存在抖动的风险
>
>- 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
>
>- 做秒杀活动时订单数据库压力过大

#### 核心流程的异步化改造

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220516175450948.png)

现在每次支付完一个订单后，都需要执行一系列的动作，包括：更新订单状态、扣减库存、增加积分、发优惠券、发短信、通知发货，这会导致一次核心链路执行时间过长，可能长达好几秒种。

**只执行最核心的 扣减库存 和 修改订单状态就可以了。**

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220516175723900.png)

代码实现

1.去除掉调用积分系统、营销系统、推送系统以及仓储系统的逻辑，而改成发送一个订单支付消息到RocketMQ里去。

2.增加rocketMQ依赖

3.生产者API

4.消费者API

>疑问：
>
>Topic的数据到底是分散在哪几个Broker上？
>
>可以分散在多少个Broker上？
>
>Producer到底是如何选择Broker发送消息过去的？

#### 第三方系统的耦合问题

发送消息的三种方式：

- 同步

- 异步

- oneway

消费消息的两种方式

RocketMQ的消费者有两个实现分别为DefaultMQPushConsumer和DefaultMQPullConsumer,它们分别为pull模式和push模式。其中pull模式为消费者主动发送请求，每隔一段时间去消息服务端拉取消息，push模式是采取长轮询的机制，消费者轮询方式主动发送请求到服务端Broker，Broker如果检测到有新的消息，则立即返回，否则暂时不返回任何消息，将请求挂起缓存到本地，Broker有一个线程检测挂起请求，等到有新消息时，对请求进项响应。


#### 订单数据同步给大数据

通过MySQL的binlog 同步到大数据，避免直接查库。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220517104044837.png)

> 公司里有没有那种其他团队要获取你们的核心数据的情况？
>
> 什么核心数据是最有价值的？

#### 秒杀系统

秒杀活动主要涉及到的并发压力就是两块，一个是高并发的读，一个是高并发的写。

首先大量用户会拿着APP不停的刷新一个秒杀商品的页面。

> 如何解决秒杀商品活动页面被同一个时间点的大量用户频繁访问，造成商品详情页系统压力过大的问题。

采取的是**页面数据静态化+多级缓存**的方案。

首先第一步，秒杀商品页面必须是将其数据做到静态化，这是什么意思呢？

简单来说是这样，如果让秒杀商品页面是动态化的，那么每次一个用户只要访问这个商品详情页，就必须发送一次请求到后端的商品详情页系统来获取数据。比如商品的标题、副标题、价格、优惠策略、库存、大量的图片、商品详情说明、售后政策等等，这一大堆的东西都是商品详情页的数据。那么你可以选择让用户浏览这个秒杀商品的时候，每次都发送请求到后台去加载这些数据过来，然后渲染出来给用户看这个商品页面，这就是所谓的动态模式。

所以首先需要将这个秒杀活动的商品详情页里的数据做成静态化的，也就是说提前就从数据库里把这个页面需要的数据都提取出来组装成一份静态数据放在别的地方，避免每次访问这个页面都要访问后端数据库。

多级缓存的架构

我们会使用CDN + Nginx + Redis的多级缓存架构。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220517115500452.png)

> 如果你们实在没有这样的场景，那么你能不能去假想一个出来？
>
> 比如你做的就是最普通的财务系统，几乎没什么高并发，但是你可以假象一下，假设你们公司的财务系统要对接大量的供应商，也许在某个特殊的时间点，比如每个月提现结算的时候，会有大量的请求到你的财务系统里来？
>
> ???

用答题的方法避免作弊抢购以及延缓下单

为秒杀独立出来一套订单系统

基于Redis实现下单时精准扣减库存

抢购完毕之后提前过滤无效请求

> 如何理解？
>
> 比如一旦商品抢购完毕，可以在ZooKeeper中写入一个秒杀完毕的标志位，然后ZK会反向通知Nginx中我们自己写的Lua脚本，通过Lua脚本后续在请求过来的时候直接过滤掉，不要向后转发了。

瞬时高并发下单请求进入RocketMQ进行削峰

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220517120904411.png)

#### 总结

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220517121414247.png)

### RocketMQ 深入

>对生产者往Broker集群发送消息的底层原理做一个研究看看Broker对于接收到的消息，到底是如何存储到磁盘上去的？
>
>基于DLedger技术部署的Broker高可用集群，到底如何进行数据同步的？
>
>消费者到底是基于什么策略选择Master或Slave拉取数据的？
>
>消费者是如何从Broker拉取消息回来，进行处理以及ACK的？
>
>如果消费者故障了会如何处理？

#### 数据分片

**Topic、MessageQueue以及Broker之间到底是什么关系？**

比如你现在有一个Topic，我们为他指定创建了4个MessageQueue，那么我们接着来思考一下，这个Topic的数据在Broker集群中是如何分布的？

所以在这里RocketMQ引入了MessageQueue的概念，本质上就是一个数据分片的机制。在这个机制中，假设你的Topic有1万条数据，然后你的Topic有4个MessageQueue，那么大致可以认为会在每个MessageQueue中放入2500条数据

**如果某个Broker出现故障该怎么办？**

通常来说建议大家在Producer中开启一个开关，就是`sendLatencyFaultEnable`.一旦打开了这个开关，那么他会有一个自动容错机制。

比如如果某次访问一个Broker发现网络延迟有500ms，然后还无法访问，那么就会自动回避访问这个Broker一段时间，比如接下来3000ms内，就不会访问这个Broker了。

这样的话，就可以避免一个Broker故障之后，短时间内生产者频繁的发送消息到这个故障的Broker上去，出现较多次数的异常。而是在一个Broker故障之后，自动回避一段时间不要访问这个Broker，过段时间再去访问他。

那么这样过一段时间之后，可能这个Master Broker就已经恢复好了，比如他的Slave Broker切换为了Master可以让别人访问了。

> Kafka、RabbitMQ有类似MessageQueue的数据分片机制吗?

#### Broker数据存储机制

Broker数据存储实际上才是一个MQ最核心的环节，他决定了生产者消息写入的吞吐量，决定了消息不能丢失，决定了消费者获取消息的吞吐量。

> 顺序写入，随机读取。

**CommitLog消息顺序写入机制**

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518105301340.png)

这个CommitLog是很多磁盘文件，每个文件限定最多1GB，Broker收到消息之后就直接追加写入这个文件的末尾，就跟上面的图里一样。如果一个CommitLog写满了1GB，就会创建一个新的CommitLog文件。

**messageQueue 在broker 存储机制中有什么作用?**

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518114313466.png)

在Broker的磁盘上，会有下面这种格式的一系列文件：

```shell
$HOME/store/consumequeue/{topic}/{queueId}/{fileName}
```

在ConsumeQueue中存储的每条数据不只是消息在CommitLog中的offset偏移量，还包含了消息的长度，以及taghashcode，一条数据是20个字节，每个ConsumeQueue文件保存30万条数据，大概每个文件是5.72MB。

**如何提高消息写入CommitLog文件性能？**

Broker是基于OS操作系统的**PageCache和顺序写**两个机制，来提升写入CommitLog文件的性能的。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518123251824.png)

顺序写：只需在文件末尾追加

PageCache：数据先写入OS的PageCache缓存中，然后后续由OS自己的线程异步化将缓存里的数据刷入磁盘中。

**同步刷盘与异步刷盘**

1. 异步刷盘可以提供超高的写入吞吐量，但是有丢失数据的风险，这个适用于什么业务场景？在你所知道的业务场景，或者工作接触过的业务场景中，有哪些场景需要超高的写入吞吐量，但是可以适度接受数据丢失？
2. 同步刷盘会大幅度降低写入吞吐量，但是可以让你的数据不丢失，你接触哪些场景，是严格要求数据务必不能丢失任何一条，但是吞吐量并没有那么高的呢？另外，大家可以去结合本节的内容，去查找资料看看，Kafka、RabbitMQ他们的broker收到消息之后是如何写入磁盘的？采用的是同步刷盘还是异步刷盘的策略？为什么？

#### 自动主从切换备灾

rocketMQ是不支持主从切换的，在4.5之后，引入基于DLedger技术的主从切换。

DLedger技术实际上首先他自己就有一个CommitLog机制，你把数据交给他，他会写入CommitLog磁盘文件里去，这是他能干的第一件事情。

所以首先我们在下面的图里可以看到，如果基于DLedger技术来实现Broker高可用架构，实际上就是用DLedger先替换掉原来Broker自己管理的CommitLog，由DLedger来管理CommitLog

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518145826655.png)

DLedger是如何从3台机器里选举出来一个Leader的？

实际上DLedger是基于Raft协议来进行Leader Broker选举的，那么Raft协议中是如何进行多台机器的Leader选举的呢？

> 投票过程：
>
> 简单来说，三台Broker机器启动的时候，他们都会投票自己作为Leader，然后把这个投票发送给其他Broker。
>
> 我们举一个例子，Broker01是投票给自己的，Broker02是投票给自己的，Broker03是投票给自己的，他们都把自己的投票发送给了别人。
>
> 此时在第一轮选举中，Broker01会收到别人的投票，他发现自己是投票给自己，但是Broker02投票给Broker02自己，Broker03投票给Broker03自己，似乎每个人都很自私，都在投票给自己，所以第一轮选举是失败的。
>
> 因为大家都投票给自己，怎么选举出来一个Leader呢？
>
> 接着每个人会进入一个随机时间的休眠，比如说Broker01休眠3秒，Broker02休眠5秒，Broker03休眠4秒。
>
> 此时Broker01必然是先苏醒过来的，他苏醒过来之后，直接会继续尝试投票给自己，并且发送自己的选票给别人。
>
> 接着Broker03休眠4秒后苏醒过来，他发现Broker01已经发送来了一个选票是投给Broker01自己的，此时他自己因为没投票，所以会尊重别人的选择，就直接把票投给Broker01了，同时把自己的投票发送给别人。
>
> 接着Broker02苏醒了，他收到了Broker01投票给Broker01自己，收到了Broker03也投票给了Broker01，那么他此时自己是没投票的，直接就会尊重别人的选择，直接就投票给Broker01，并且把自己的投票发送给别人。此时所有人都会收到三张投票，都是投给Broker01的，那么Broker01就会当选为Leader。
>
> **核心机制就是一轮选举不出来Leader的话，就让大家随机休眠一下，先苏醒过来的人会投票给自己，其他人苏醒过后发现自己收到选票了，就会直接投票给那个人。**

DLedger是如何基于Raft协议进行多副本同步的？

简单来说，数据同步会分为两个阶段，一个是uncommitted阶段，一个是commited阶段

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518154013523.png)

Follower Broker的DLedgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker的DLedgerServer，然后如果Leader Broker收到超过半数的Follower Broker返回ack之后，就会将消息标记为committed状态。

然后Leader Broker上的DLedgerServer就会发送commited消息给Follower Broker机器的DLedgerServer，让他们也把消息标记为comitted状态。

如果Leader Broker崩溃了怎么办？

- 重新选举

- 新选举出来的Leader会把数据通过DLedger同步给剩下的一个Follower Broker。

#### 消费者获取消息

**消费组到底是个什么概念？**

消费者组的意思，就是让你给一组消费者起一个名字。

**不同消费者之间的关系**

如果不同的消费组订阅了同一个Topic，对Topic里的一条消息，每个消费组都会获取到这条消息。

对于每个消费者组的不同机器，取决于使用集群模式还是广播模式：

- 集群模式，同一条消息只有一个消费者机器会收到，自动负载均衡。
- 广播模式，都会收到。

> 回顾之前学习到的知识，我们大致可以如此理解，
>
> Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。

**MessageQueue与消费者的关系**

接着我们来想一个问题，对于一个Topic上的多个MessageQueue，是如何由一个消费组中的多台机器来进行消费的呢？

其实这里的源码实现细节是较为复杂的，但我们可以简单的理解为，他会均匀的将MessageQueue分配给消费组的多台机器来消费。

这里的一个原则就是，一个MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理。

**Push模式 vs Pull模式**

挂起和长轮询的机制 ……

**Broker是如何将消息读取出来返回给消费机器的？**

假设一个消费者机器发送了拉取请求到Broker了，他说我这次要拉取MessageQueue0中的消息，然后我之前都没拉取过消息，所以就从这个MessageQueue0中的第一条消息开始拉取好了。

于是，Broker就会找到MessageQueue0对应的ConsumeQueue0，从里面找到第一条消息的offset，如下图所示。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518162107568.png)

接着Broker就需要根据ConsumeQueue0中找到的第一条消息的地址，去CommitLog中根据这个offset地址去读取出来这条消息的数据，然后把这条消息的数据返回给消费者机器。

所以其实消费消息的时候，本质就是根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

**消费者机器如何处理消息、进行ACK以及提交消费进度？**

消费者机器拉取到一批消息之后，就会将这批消息回调我们注册的一个函数，如下面这样子：

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518162313941.png)

当我们处理完这批消息之后，消费者机器就会提交我们目前的一个消费进度到Broker上去，然后Broker就会存储我们的消费进度比如我们现在对ConsumeQueue0的消费进度假设就是在offset=1的位置，那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度。

那么下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

**如果消费组中出现机器宕机或者扩容加机器，会怎么处理？**

进入一个rabalance的环节，也就是说重新给各个消费机器分配他们要处理的MessageQueue。

> 一般我们获取到一批消息之后，什么时候才可以认为是处理完这批消息了？
>
> 是刚拿到这批消息就算处理完吗？
>
> 还是说要对这批消息执行完一大堆的数据库之类的操作，才算是处理完了？
>
> 如果获取到了一批消息，还没处理完呢，结果机器就宕机了，此时会怎么样？
>
> 这些消息会丢失，再也无法处理了吗？
>
> 如果获取到了一批消息，已经处理完了，还没来得及提交消费进度，此时机器宕机了，会怎么样呢？

**消费者到底是根据什么策略从Master或Slave上拉取消息的**

当你拉取消息的时候，可以轻松从os cache里读取少量的ConsumeQueue文件里的offset，这个性能是极高的，但是当你去CommitLog文件里读取完整消息数据的时候，会有两种可能。

第一种可能，如果你读取的是那种刚刚写入CommitLog的数据，那么大概率他们还停留在os cache中，此时你可以顺利的直接从oscache里读取CommitLog中的数据，这个就是内存读取，性能是很高的。

第二种可能，你也许读取的是比较早之前写入CommitLog的数据，那些数据早就被刷入磁盘了，已经不在os cache里了，那么此时你就只能从磁盘上的文件里读取了，这个性能是比较差一些的。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518181025158.png)

> 总结：
>
> 本质是对比你当前没有拉取消息的数量和大小，以及最多可以存放在os cache内存里的消息的大小，如果你没拉取的消息超过了最大能使用的内存的量，那么说明你后续会频繁从磁盘加载数据，此时就让你从slave broker去加载数据了！
>
> **即减少去磁盘读取commitLog。**

#### 高性能网络通信架构

**Reactor主线程与长短连接**

Reactor”是个什么东西?

>一句话解释一下短连接：如果你要给别人发送一个请求，必须要建立连接 -> 发送请求 -> 接收响应 -> 断开连接，下一次你要发送请求的时候，这个过程得重新来一遍每次建立一个连接之后，使用这个连接发送请求的时间是很短的，很快就会断开这个连接，所以他存在时间太短了，就是短连接。
>
>长连接的话，就是反过来的意思，你建立一个连接 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应大家会发现，当你建立好一个长连接之后，可以不停的发送请求和接收响应，连接不会断开，等你不需要的时候再断开就行了，这个连接会存在很长时间，所以是长连接。

**Producer和Broker建立一个长连接**

在Broker里用什么东西代表跟Producer之间建立的这个长连接呢？答案是：SocketChannelProducer里面会有一个SocketChannel，Broker里也会有一个SocketChannel，这两个SocketChannel就代表了他们俩建立好的这个长连接。

**基于Reactor线程池监听连接中的请求**

Producer发送请求过来了，他发送一个消息过来到达Broker里的SocketChannel，此时Reactor线程池里的一个线程会监听到这个SocketChannel中有请求到达了！

**基于Worker线程池完成一系列准备工作**

Reactor线程从SocketChannel中读取出来一个请求，这个请求在正式进行处理之前，必须就先要进行一些准备工作和预处理，比如SSL加密验证、编码解码、连接空闲检查、网络连接管理，诸如此类的一些事那么问题又来了，这些事让谁来干呢？这个时候需要引入一个新的概念，叫做Worker线程池，他默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中的一个线程进行处理，会完成上述一系列的准备工作。

**基于业务线程池完成请求的处理**

接收到了消息，肯定是要写入CommitLog文件的，后续还有一些ConsumeQueue之类的事情需要处理，类似这种操作，就是业务处理逻辑。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220518190003674.png)

**为什么这套网络通信框架会是高性能以及高并发的？**

专门分配一个Reactor主线程出来，就是专门负责跟各种Producer、Consumer之类的建立长连接。

一旦连接建立好之后，大量的长连接均匀的分配给Reactor线程池里的多个线程。

每个Reactor线程负责监听一部分连接的请求，这个也是一个优化点，通过多线程并发的监听不同连接的请求，可以有效的提升大量并发请求过来时候的处理能力，可以提升网络框架的并发能力。

接着后续对大量并发过来的请求都是基于Worker线程池进行预处理的，当Worker线程池预处理多个请求的时候，Reactor线程还是可以有条不紊的继续监听和接收大量连接的请求是否到达。

而且最终的读写磁盘文件之类的操作都是交给业务线程池来处理的，当他并发执行多个请求的磁盘读写操作的时候，不影响其他线程池同时接收请求、预处理请求，没任何的影响。

> BIO、NIO、AIO以及Netty之间的关系是什么？

#### 基于mmap内存映射实现磁盘文件的高性能读写

解决多次数据拷贝问题。

首先，假设我们有一个程序，这个程序需要对磁盘文件发起IO操作读取他里面的数据到自己这儿来，那么会经过以下一个顺序：

首先从磁盘上把数据读取到**内核IO缓冲区**里去，然后再从内核IO缓存区里读取到用户进程私有空间里去，然后我们才能拿到这个文件里的数据。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220519110708238.png)

传统普通IO的问题，有两次数据拷贝问题。

首先，RocketMQ底层对CommitLog、ConsumeQueue之类的磁盘文件的读写操作，基本上都会采用mmap技术来实现。如果具体到代码层面，就是基于JDK NIO包下的**MappedByteBuffer的map()**函数，来先将一个磁盘文件（比如一个CommitLog文件，或者是一个ConsumeQueue文件）映射到内存里来这里我必须给大家解释一下，这个所谓的内存映射是什么意思。

刚开始你建立映射的时候，并没有任何的数据拷贝操作，其实磁盘文件还是停留在那里，只不过他把物理上的磁盘文件的一些地址和用户进程私有空间的一些虚拟内存地址进行了一个映射。

mmap技术在进行文件映射的时候，一般有大小限制，在1.5GB~2GB之间所以RocketMQ才让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大。

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220519115109451.png)

如果我们要从磁盘文件里读取数据呢？那么此时就会判断一下，当前你要读取的数据是否在PageCache里？

如果在的话，就可以直接从PageCache里读取了！比如刚写入CommitLog的数据还在PageCache里，此时你Consumer来消费肯定是从PageCache里读取数据的。

但是如果PageCache里没有你要的数据，那么此时就会从磁盘文件里加载数据到PageCache中去，如下图而且PageCache技术在加载数据的时候，还会将你加载的数据块的临近的其他数据块也一起加载到PageCache里去。

**预映射机制 + 文件预热机制**

（1）内存预映射机制：Broker会针对磁盘上的各种CommitLog、ConsumeQueue文件预先分配好MappedFile，也就是提前对一些可能接下来要读写的磁盘文件，提前使用MappedByteBuffer执行map()函数完成映射，这样后续读写文件的时候，就可以直接执行了。

（2）文件预热：在提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取尤其是CommitLog、ConsumeQueue的时候，其实有可能会频繁的从磁盘里加载数据到内存中去。所以其实在执行完map()函数之后，会进行madvise系统调用，就是提前尽可能多的把磁盘文件加载到内存里去。

> 如何学好一个新的技术？
>
> （1）入门和了解：官网文档大致看一下，网上一些入门博客看一下，然后写几个demo出来，对基本原理有一定的了解，这个就是入门的水平了
>
> （2）熟悉：当了解一个技术之后，接着要做的事情，务必是实践和做项目，必须由项目或者场景驱动，去思考一个技术到底应该怎么来使用如果手头有一个项目可以用这个技术，那是最好的，如果没有的话，那么可以关注一些实战类的专栏或者课程，像狸猫技术窝的专栏，我们一般都会从案例场景驱动，去讲解技术如何实践当你对一个技术有了一定的实践经验，或者从一些资料中吸收了一些实践经验之后，往往这个时候你会对这个技术的实践方案、部署架构、生产方案以及优化方式，有了一定的了解了，这个时候就是熟悉的水平了。
>
> （3）精通：当你对一个技术有了较多的实践之后，可以选择对这个技术的源码相关的博客、书籍进行阅读，甚至直接阅读其源码，当你精通一个技术的源码之后，可以称之为对这个技术掌握到了精通
>
> （4）顶尖专家：如果你对某个技术有很深的研究，在开源社区里是核心的commitor，在一个大公司里是这个技术的最高负责人，基于这个技术抗下了超高的负载，有了行业里第一手的顶尖实践经验，而且对底层有深入的研究，此时你可以称之为是这个技术在国内的顶尖专家。

#### 总结

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220519115830383.png)

### 实战方案

#### 消息丢失

**从 RocketMQ 全链路分析一下为什么用户支付后没有收到现金红包？**

因网络抖动的生产者投递失败，MQ本身的宕机导致内存消息丢失，主从切换导致的消息丢失，磁盘损坏导致的。消费者自动提交offset等。

**如何保证消息不丢失呢？**

1. 可靠性投递：利用事务消息，先发送半消息到broker，监听本地事务，事务提交之后，broker再把消息提交。（这里的问题是先发送消息还是先查数据库）
2. broker同步刷盘（os cache）+主从切换(raft)
3. consumer 手动提交（引出重复消费，接口幂等）+ 故障转移



**事务机制的底层原理（参考MQ的事务消息篇）**



**异步刷盘**

调整broker的配置文件，将其中的flushDiskType配置设置为：SYNC_FLUSH，默认他的值是ASYNC_FLUSH。

**主从架构解决磁盘损坏**

基于DLedger技术和Raft协议的主从同步架构。

**手动提交**

![](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220727174818446.png)

**消息零丢失方案到底适合什么场景？**

支付、订单等重要场景。

#### 重复消费

系统重启，接口超时，导致重复消费是一个很常见的问题，一定要考虑进去。

生产端：发送前先去MQ查一遍，没有再发送；每次发送同时写入redis，发的时候查redis，不保险。

消费端：幂等

#### 死信队列

如果消费者的数据库宕机了，那消息无法消费，offset无法提交，broker就会一直重试，这时消费就会阻塞，造成消息堆积。怎么解决这种问题呢？RocketMQ为我们设计了死信队列。

![image-20220727175022374](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220727175022374.png)

这里我们捕获异常，并告诉broker，稍后我还要再消费一次。那broker如何存储待消费的消息？按什么机制再次重试。

broker为每个消费组分配一个重试队列，比如你的消费组的名称是“VoucherConsumerGroup”，意思是优惠券系统的消费组，那么他会有一个“%RETRY%VoucherConsumerGroup”这个名字的重试队列，

然后过一段时间之后，重试队列中的消息会再次给我们，让我们进行处理。如果再次失败，又返回了RECONSUME_LATER，那么会再过一段时间让我们来进行处理，默认最多是重试**16**次！每次重试之间的间隔时间是不一样的，这个间隔时间可以如下进行配置：

> messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h

那超过16次怎么处理呢？

这时候死信队列登场，即死掉的消息。死信队列的名字是“%DLQ%VoucherConsumerGroup”。

死信队列怎么处理？

可以后台开一个线程订阅这个队列，对里面的消息再次消费或者存入musql作为失败数据。

> 为什么不自己重试消费？
>
> 因为会阻塞消费者，再offset提交之前是无法消费下一批消息的。如果放在内存里异步重试，会有丢消息的风险。

#### 顺序消费

场景：订单消息同步到大数据服务

<img src="https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220728091102821.png" style="zoom:50%;" />

1. 让属于同一个订单的binlog进入一个MessageQueue （id取模发送到指定队列）
2. 保证发送的有序
3. Consumer有序处理一个订单的binlog（一个Consumer可以处理多个MessageQueue的消息，但是一个MessageQueue只能交给一个Consumer来进行处理，）
4. 消息不能进入重试队列，返回 SUSPEND_CURRENT_QUEUE_A_MOMENT

**代码实现**

指定队列发送



#### 数据过滤

我们可以在发送时设置tag或其他自定义属性：

![image-20220728105918408](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220728105918408.png)

消费时就可以过滤掉不想要的消息：

![image-20220728105957817](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220728105957817.png)

过滤语法：

![image-20220728105825794](https://yitiaoit.oss-cn-beijing.aliyuncs.com/img/image-20220728105825794.png)

#### 延迟消息

生产案例：延迟消息处理订单超时关闭





#### 集群权限管理



#### 消息轨迹追踪



#### 消息积压





#### 消息限流



#### kafka到rocket的迁移



### 源码解读







